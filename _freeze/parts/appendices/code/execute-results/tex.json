{
  "hash": "f8bca285fa8dca7558212cf699fb831b",
  "result": {
    "markdown": "---\nexecute:\n  freeze: auto\n---\n\n\n# Code and Computation\n\n## Appia\n\nModern structural biology makes extensive use of size-exclusion chromatography (SEC).\nThe two types of SEC I encountered most often were *analytic chromatography*, in which samples are assessed for monodispersity and relative amount; and *preparative chromatography*, in which samples are purified based on their size.\nIn an environment where only a heuristic assessment of these parameters is important (e.g., assessing whether a sample is homogeneous enough to make grids of, or whether a protein interacts with another) these parameters can be assessed by eye (@fig-appia-peaks).\nHowever, chromatography instrument manufacturers must include capacity for more complex analyses in their software.\nThis necessarily makes the software more complex and arcane in the service of features that biochemists rarely need.\nTo solve this mismatch I created Appia, a web-based chromatography visualization package [@posert2023].[Appia can be installed via `pip`, or the source can be inspected at [my github](https://github.com/plethorachutney/appia).]{.aside}\nIn this section I will provide a brief summery of Appia's functionality and intended use, the installation process, and then some guidance for adding support for new chromatography instruments.\n\n![Heuristic chromatography parameters are easily assessed by eye. Users can approximate affinity for the stationary phase (size, in the case of SEC), quantity, homogeneity, and the number of distinct species by inspecting peak shifts, peak height, peak width, and number of peaks respectively.](figures/code/peaks.png){#fig-appia-peaks}\n\n### Structure\n\nAppia is divided into two conceptual parts: the local processor and the web interface.\nThe local processor is written entirely in python and processes all supported data formats into a unified Appia experiment.\nSeveral options are available at processing time, such as applying a scaling factor for various manufacturers or automatic trace plotting.\nAdditionally, the local processor includes a user settings system.\nThis system stores information such as flow rates to apply to a specific chromatography method and database login information.\nExperiments, to Appia, are collections of analytic and preparative traces.\nImportantly, Appia is agnostic as to the origins of the data in a single experiment &mdash; experiments may contain traces from any number of manufacturers and/or instruments, provided Appia has a processor for that manufacturer.\nThe experiments are stored on the local machine in both long (appropriate for ggplot2 and other grammar-of-graphics plotting systems) and wide (appropriate for, e.g., Excel or Prism) formats (@fig-appia-tidy).\nThese experiments are packaged with an experiment ID and uploaded to the other half of Appia, Appia Web.\n\n![Data formats saved by Appia and available for user download. Traces made up of individual samples (A) are represented as a single row in the long format (B). The wide format (C) instead represents the independent variable with its own column (Time, red), the response variable with the table body (Signal, white), and all other variables in column headers (various colors).](figures/code/tidy-explainer.png){#fig-appia-tidy}\n\nAppia Web provides a browser-based GUI and centralized trace database.\nThe database is a CouchDB database containing a simple dictionary of experiments stored under their experiment IDs.\nThe browser-based GUI is written in python and plotly dash, and loads user-requested data from the database and presents it using a series of line plots.\nAnalytic data is presented with one line per sample per channel, while preparative data is presented as a single line per injection with fractions highlighted by fill.\nAnalytic data is also presented with a normalization applied, such that the maximum of each trace is 1 and the minimum is 0.\nThe user may select a region of the analytic traces and re-normalize them to set the maximum over that range to 1, scaling the rest of the trace linearly (@fig-appia-normalization).\nThis normalization process is intended to aid in comparison of relative peak heights between different samples which may have a different total amount of material, or to compare peaks within a trace compared to some standard peak height.\nImportantly, this process uses the simple readout of peak height.\nMore complex analyses or analyses requiring quantitative accuracy are outside the scope of Appia and should use manufacturer-provided peak calling and fitting.\nUsers can also download images and raw data from Appia Web.\n\n![Appia's normalization process. A user-selected region is used to linearly scale all traces such that their maximum value over that region is 1.](figures/code/normalization.png){#fig-appia-normalization}\n\n### Installation\n\nAppia can be used solely to locally process chromatography data into a standard format for later plotting.\nHowever, we recommend installing a processing installation on each instrument computer and a single centralized Appia Web installation on some networked computer.\nThis allows for single-click processing and upload of data to a central repository which users can then inspect, manipulate, and download from their personal computers without installing Appia.\nTo facilitate this process, I have created Docker images for Appia Web.\nImages are available for both x86-64 and ARM architectures, and docker-compose templates are available in the project repository.\nRunning one of these templates creates and networks the database and Appia Web automatically, requiring only that the user expose the correct ports (5984 for CouchDB and 8080 for Appia Web).\n\nBelow is an example process for installing Appia Web on a Windows PC which is accessible at `example.ohsu.edu`:\n\n 1. Install docker desktop\n 2. Download `install-appia-web_pc.ps1` and run it. Provide the desired configuration information\n 3. Run `docker-compose up -d`\n 4. Ensure that ports 5984 (database) and 8080 (web interface) are accessible\n\nThe Appia Web interface would now be available at example.ohsu.edu:8080/traces. All that is left is installing the processor on the instrument computer, a much simpler process:\n\n 1. Install python\n 2. Run `python -m pip install appia`\n 3. Run `appia utils --database-setup` and provide the information set up during Appia Web installation.\n\nNow when a user finishes a chromatography experiment, they first export it to whatever format Appia expects (typically the default text format) and run (assuming the files end in `.txt`) `appia process *.txt -d` to upload the data to the web interface. The files will automatically be organized and stored on the local computer as well. This command can be wrapped in a script to avoid the need for users to interact with the command line.\n\n### Peak fitting with Gaussian Mixture Models\n\nOne useful form of analysis that Appia cannot perform on its own, but that can be performed with Appia's data output, is a Gaussian Mixture Model (GMM).\nIn GMMs, we (perhaps obviously) treat our signal as a mixture of pure gaussian processes.\nThis is especially useful when analyzing chromatography of species with known retention times.\nFor instance, suppose that we know that a Fab binds a target, which shifts the target's retention time from 10 minutes to 9.8 minutes.\nThe Fab's retention time is well outside this region, perhaps at 24 minutes, and so we can ignore unbound Fab.\nIf we mix these proteins and look at the signal around 10 minutes, we might observe a peak like @fig-example-data.\nWe want to know what proportion of the target has shifted to the 9.8 minute retention time, since that will tell us how much has bound.\nHowever, it's impossible to tell this by looking at the data, or even by using manufacturer-written peak integrating algorithms.\nHowever, since we know the retention times of each component, we can model the mixture ourselves.\nI'll work this example in R, but it could just as easily be done in python, Matlab, or whatever modeling software the reader prefers.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Example data of a mixture between a protein and a Fab.](code_files/figure-pdf/fig-example-data-1.pdf){#fig-example-data}\n:::\n:::\n\n\n\nGaussian curves have two components: the mean and the standard deviation.\nIn a chromatography context, the mean is the retention time and the standard deviation the monodispersity of the peak (i.e., its width).\nWe also need to model the amount of the component, which we can model with a simple scalar for each component.\nPerforming a non-linear least squares fit to the data should give us these parameters\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ub for unbound, b for bound\nub_rt <- 10\nb_rt <- 9.8\n\n# nls needs initial starting values,\n# which we can eyeball.\n# In this case it looks like our\n# peak is mostly the unbound part\nguesses <- list(\n    'ub_amount' = 0.8, # real: 1.0\n    'b_amount' = 0.4, # real: 0.3\n    'ub_sd' = 0.15, # real: 0.1\n    'b_sd' = 0.3 # real: 0.2\n)\n\nresults <- nls(\n    # This is the mixture model\n    formula = \n        signal\n        ~ ub_amount * dnorm(\n            rt, mean = ub_rt, sd = ub_sd\n        )\n        + b_amount * dnorm(\n            rt, mean = b_rt, sd = b_sd\n        ),\n    data = example_trace,\n    start = guesses\n)\n```\n:::\n\n\n\nThe `results` variable now contains the fitted values for each component.\nTo see how well the data have been fit, we can plot each component separately, or their sum, as in @fig-fitted-gauss.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# helper function to extract values\nget_val <- function(val_name) {\n    coef(results)[[val_name]]\n}\n# function generator for the gaussians\nplot_gauss <- function(scale, mean, sd) {\n    \\(x) scale * dnorm(x, mean = mean, sd = sd)\n}\nfit_unbound <- plot_gauss(\n            get_val('ub_amount'),\n            ub_rt, get_val('ub_sd')\n        )\nfit_bound <- plot_gauss(\n            get_val('b_amount'),\n            b_rt, get_val('b_sd')\n        )\n\ndata_plot + \n    geom_function(\n        fun = fit_unbound,\n        color = 'blue',\n        linetype = 'dotted',\n        n = 1000,\n        linewidth = 1\n    ) +\n    geom_function(\n        fun = fit_bound,\n        color = 'forestgreen',\n        linetype = 'dotted',\n        n = 1000,\n        linewidth = 1\n    ) +\n    geom_function(\n        fun = \\(x) fit_unbound(x) + fit_bound(x),\n        color = 'black',\n        n = 1000\n    )\n```\n\n::: {.cell-output-display}\n![Fitted values plotted atop data. Bound and unbound are green and blue respectively, with the additive model in black.](code_files/figure-pdf/fig-fitted-gauss-1.pdf){#fig-fitted-gauss fig-pos='H'}\n:::\n:::\n\n\n\nWe see the data fit well &mdash; we now have a model for the components of our experiment.\nTo assess, for instance, the amount of protein in each peak, we could from here use the well-known equations for the area of a gaussian to find the peak area for each population, or just take their height as a heuristic.\nIf this simple sum of gaussians model does not fit, you could consider adding a constant (for a constant non-zero baseline) or parameters for a linear function (for a linearly changing baseline) to the fit formula.\n\n### Writing a new processor\n\n*This section is intended for the advanced reader who wishes to add support for a chromatography instrument to Appia.*\n\nProcessors are each a class which inherits from either `HplcProcessor` or `FplcProcessor` (for analytic or preparative data, respectively).\nAt the time of writing, Appia supports the following data formats:\n\n * Waters Empower `.arw` files\n * Shimadzu `.asc` files\n * Shimadzu `.txt` files\n * Agilent `.csv` files\n * Cytiva/GE `.csv` files\n\nYour processor can perform any necessary setup in `__init__()`, but it must eventually run\n\n``` python\nsuper().__init__(\n    filename,\n    manufacturer = {appropriate_string},\n    **kwargs\n)\n```\n\nWhen Appia processes files, for each file, it creates an instance of every `Processor` child.\nThese `Processors` all decide whether they can process a file using their `claim_file()` method, which you must implement.\nThis method should accept a filename and return `True` if the processor thinks that is a type of file it can process and `False` otherwise.\nFor instance, here is `WatersProcessor.claim_file()`:\n\n```python\n@classmethod\ndef claim_file(cls, filename) -> bool:\n    return filename[-4:].lower() == '.arw'\n```\n\nIt simply checks whether the file ends in `.arw`.\nDepending on the data format for the new manufacturer, the method may require more complex checks, or you may have to add more specificity to other `claim_file()` methods.\n\nIf your processor claims the file, it will *automatically* run two methods: `prepare_sample()` and `process_file()`.\nThe distinction between these is arbitrary, as they are both always run in sequence.\nIt is intended that `prepare_sample()` collects metadata about the sample (method name, flow rate, sample set name, etc.) and `process_file()` creates the actual data frame.\nThe data frame *must be in the long format*, should be stored in your processor's `df` attribute, and should have exactly the following column names:\n\n * **mL**: the retention volume in mL.\n * **Time**: the retention time in minutes.\n * **Sample**: the sample name.\n * **Channel**: the signal channel. Values in this column are arbitrary but should follow some kind of internal logic.\n * **Normalization**: whether this row is normalized (\"Normalized\") or raw (\"Signal\").\n * **Value**: the y-axis position for this row.\n\nAt this point, your `Processor` is done.\nAs long as only one `Processor` claimed a file, the Appia `parser` will handle concatenation, plotting, and upload for you.[I would encourage you to write a test as well, of course.]{.aside}\n\nThe parent classes provide a few conveniences for you.\nFor example, the `flow_rate` and `column_volume` getters will prompt the user for the relevant info, if necessary, and then offer to store it in the user settings.\nIt will also handle overrides of various parameters, which are accepted through `kwargs`.\nAdditionally, `appia.processors.core` provides the `normalize()` function, which handles trace normalization.\nI encourage you to take a look at the `Processors` I've already written before embarking on writing your own.\n\n## Model-building tools\n\nI've written a few tools to aid with model building and general structure analysis.\nThey're all available [on my github](https://github.com/plethorachutney/model-building-tools).\n\n:::{.column-margin}\n![An image from `ca_bilder.py` highlighting the movement of the finger and thumb between the resting and desensitized states of ASIC1.](figures/code/ca-bilder.png){#fig-ca-bilder}\n:::\n\n`ca_bilder.py` creates a `.bild` file visualizing C&alpha; shifts between two given models.\nCylinders are drawn between each pair of C&alpha; atoms, colored by the distance.\nThe colors are scaled per-model, with the darkest color corresponding to the greatest shift.\nThe models must be aligned before they are run through the script.\nIf the models are multi-chain, cylinders will be drawn between the same chain ID.\nIf the models are of two different proteins, you must provide a FASTA alignment and indicate which chain belongs to which sequence.\nThese visualizations can be useful to highlight when certain domains move while others stay still, such as ASIC1's gating cycle (@fig-ca-bilder).\n\n`check_for_glyc.py` searches a PDB model for the sequence NXS/T, the canonical N-linked glycosylation sequence.\nAll asparagines matching this requirement are added to a ChimeraX command HTML file.\nOpening this file with ChimeraX creates a list of clickable asparagine names.\nClicking these links orients the camera on the appropriate residue and removes the link from the list, creating a checklist of sorts.\nIf the model and map are both open, this is a convenient way to step through all potential glycosylation sites in a model and assess whether they are indeed glycosylated.\nThe user may then add sugars via the normal commands.\n\n`model_map.py`, `orientation_plotter.py`, `phenix-table-reader.py`, and the three R scripts all read information from PHENIX or 3DFSC output and produce more aesthetically appealing plots.\n`phenix-table-reader.py` is a convenient way to automatically update a Table 1 (for instance, @tbl-model-building-refinement) during the model building process.\nOutput from the other scripts can be seen in @sec-validation.\n\n`mutator.py` checks the sequence of a model against a FASTA sequence and alerts the user to any mismatches.\nIt also creates a ChimeraX command file which can be run to mutate the model to match the FASTA file.\nAny known deviations can be passed to the script as a `.txt` file with one mutation per line in a ChimeraX-like format.\nFor instance, if chain A has a C&rarr;K mutation at position 260, the `.txt` file should have a line reading `/A:C260K`.\n\n`translator.py` is currently ENaC specific, but could be generalized by a user.\nIt is a simple script to translate residue numbers between the same gene of different organisms using a FASTA file.\nIt is mostly useful when reading or writing and trying to incorporate data from multiple organisms.\nAn example of this script in use:\n\n```\n$ translator.py mA260\nMouse αK260 => Human αK233\n```\n\nAt time of writing, PHENIX adds waters to a seemingly random chain (perhaps based on proximity?) and with a number overlapping other residue numbers.\nThis makes analysis extremely frustrating, since waters have the exact same atom specifier as other residues and are spread over the chains.\n`water_fixer.py` solves this problem by reading through a PDB file and moving all waters to chain H and numbering them starting at 1.\nThis has the unfortunate side effect of waters not being numbered in a spatially coherent way, but it's better than nothing.",
    "supporting": [
      "code_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}